"""
Build FAISS Index for Q&A Knowledge Base
Chunks documents by Q&A pairs and creates vector embeddings using FastEmbed.
"""

import numpy as np
import faiss
from fastembed import TextEmbedding
from typing import List, Dict
import pickle
import os
from pathlib import Path


class QAIndexBuilder:
    """Builds FAISS index from Q&A formatted documents using FastEmbed."""
    
    def __init__(self, model_name: str = "BAAI/bge-small-en-v1.5"):
        print(f"Loading FastEmbed model: {model_name}")
        print("ğŸ’¡ Using FastEmbed with ONNX Runtime - lightweight, no PyTorch needed!")
        self.model = TextEmbedding(model_name=model_name)
        self.dimension = 384  # Dimension for bge-small-en-v1.5
        print("Model loaded successfully!")
    
    def chunk_qa_document(self, text: str, doc_name: str) -> List[Dict]:
        """
        Chunk Q&A document by Q:A: pairs.
        Each Q&A pair becomes one chunk.
        """
        chunks = []
        lines = text.split('\n')
        
        current_question = ""
        current_answer = ""
        
        for line in lines:
            line = line.strip()
            
            # Skip empty lines
            if not line:
                continue
            
            # Check if this is a question
            if line.startswith('Q:'):
                # Save previous Q&A if exists
                if current_question and current_answer:
                    qa_text = f"{current_question}\n{current_answer}"
                    chunks.append({
                        'text': qa_text.strip(),
                        'chunk_id': len(chunks),
                        'question': current_question.replace('Q:', '').strip(),
                        'answer': current_answer.replace('A:', '').strip(),
                        'document': doc_name
                    })
                
                # Start new Q&A
                current_question = line
                current_answer = ""
                
            elif line.startswith('A:'):
                # This is an answer
                current_answer = line
                
            elif current_answer:
                # Continuation of answer (multi-line)
                current_answer += " " + line
        
        # Add the last Q&A pair
        if current_question and current_answer:
            qa_text = f"{current_question}\n{current_answer}"
            chunks.append({
                'text': qa_text.strip(),
                'chunk_id': len(chunks),
                'question': current_question.replace('Q:', '').strip(),
                'answer': current_answer.replace('A:', '').strip(),
                'document': doc_name
            })
        
        return chunks
    
    def build_index_from_documents(self, documents_dir: str, output_dir: str):
        """
        Build FAISS index from all Q&A documents in the directory.
        """
        documents_path = Path(documents_dir)
        output_path = Path(output_dir)
        
        # Create output directory if it doesn't exist
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Find all .txt files in documents directory
        txt_files = list(documents_path.glob("*.txt"))
        
        if not txt_files:
            print(f"âŒ No .txt files found in {documents_dir}")
            return False
        
        print(f"\nğŸ“š Found {len(txt_files)} document(s) to process")
        
        all_chunks = []
        
        # Process each document
        for txt_file in txt_files:
            print(f"\nğŸ“„ Processing: {txt_file.name}")
            
            with open(txt_file, 'r', encoding='utf-8') as f:
                text = f.read()
            
            chunks = self.chunk_qa_document(text, txt_file.stem)
            all_chunks.extend(chunks)
            
            print(f"   âœ… Created {len(chunks)} Q&A chunks")
        
        print(f"\nğŸ“Š Total chunks created: {len(all_chunks)}")
        
        if not all_chunks:
            print("âŒ No chunks created. Check your document format.")
            return False
        
        # Extract text for embedding
        texts = [chunk['text'] for chunk in all_chunks]
        
        print("\nğŸ§  Generating embeddings with FastEmbed...")
        # Add passage prefix for better retrieval performance
        prefixed_texts = [f"passage: {text}" for text in texts]
        
        # FastEmbed returns an iterator
        embeddings_list = list(self.model.embed(prefixed_texts, batch_size=32))
        embeddings_np = np.array(embeddings_list, dtype='float32')
        
        print("ğŸ“ Normalizing embeddings...")
        # Normalize for Inner Product similarity
        norms = np.linalg.norm(embeddings_np, axis=1, keepdims=True)
        norms = np.where(norms == 0, 1, norms)
        embeddings_np = embeddings_np / norms
        
        print("ğŸ”¨ Creating FAISS index...")
        dimension = embeddings_np.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner Product for cosine similarity
        index.add(embeddings_np)
        
        print(f"âœ… Index created with {index.ntotal} vectors")
        
        # Save everything
        self.save_index(index, texts, all_chunks, output_path)
        
        print(f"\nğŸ‰ Index building completed successfully!")
        print(f"ğŸ“ Saved to: {output_path}")
        
        return True
    
    def save_index(self, index, documents, metadata, output_dir: Path):
        """Save the FAISS index and metadata."""
        
        # Save FAISS index
        index_path = output_dir / 'index.faiss'
        faiss.write_index(index, str(index_path))
        print(f"   ğŸ’¾ Saved FAISS index: {index_path}")
        
        # Save metadata
        metadata_path = output_dir / 'metadata.pkl'
        with open(metadata_path, 'wb') as f:
            pickle.dump({
                'documents': documents,
                'metadata': metadata
            }, f)
        print(f"   ğŸ’¾ Saved metadata: {metadata_path}")


def main():
    """Build the index from Q&A documents."""
    
    print("\n" + "="*60)
    print("ğŸš€ FAISS INDEX BUILDER FOR Q&A KNOWLEDGE BASE")
    print("="*60)
    
    # Paths relative to backend directory
    documents_dir = "../knowledgebase/documents"
    output_dir = "../knowledgebase/vector_store"
    
    # Build the index
    builder = QAIndexBuilder()
    success = builder.build_index_from_documents(documents_dir, output_dir)
    
    if success:
        print("\nâœ… Ready to use! The knowledge base is now searchable.")
    else:
        print("\nâŒ Failed to build index. Please check your documents.")


if __name__ == "__main__":
    main()
